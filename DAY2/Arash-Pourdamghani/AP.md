# Towards A Dynamically Optimal Self-Adjusting Network

شبکه های خودتنطیم شونده براساس اتفاقات به دو دسته تقسیم می‌شوند :

برخی به صورت آنلاین یا برخط نیازمندی ها رو تنظیم می‌کنن .

در آخر این ارائه باید جواب سه تا سوال رو بدین چرا اصلا به شبکه های خود تنظیم شونده نیاز داریم؟ و اینها چطور طراحی و تنظیم میشن؟
ما توی این ارائه دو تا مقاله که منتشر شده رو مرور می‌کنیم و حدود یه ماه دیگه هر کدوم از این مقاله ها منتشر میشه.

چرا اصلا شبکه های خود تنظیم شونده ؟ و به چه دردی میخوره ؟

احتمالا همه قانون مورف رو آشنا هستین هر هجده ماه یه بار قدرت پردازشی رو دو‌برابر می‌کنیم اما ما تا سال ۲۰۲۵ داریم به پایان شیوه سنتی قانون مورف نزدیک میشیم. ولی از اون طرف نیاز و درخواست برای پهنای باند نیاز هست که اینها بیشتر و بیشتر میشه مخصوصا کارهای جدیدی که اومده نیاز به پهنای باند  بیشتری داره و با بهبود الگوریتم ها و نرم افزار ها می‌خوایم این نیاز رو برطرف کنیم و روش های بهتری ابداع کنیم حالا ایده اینکه چرا اصلا میتونیم جلو بریم از چه قرار هست ؟ می‌خوایم در این مورد صحبت کنیم.


توسط اطلاعات و درخواست هایی که رد و بدل میشه بین سرور های مختلف، دستگاه های مختلف کاملا تصادفی و یکی یکی به هم وصل نمیشن اگه میخواین ببینین چه اتفاقی می‌افته یه سری پترن های خاصی وجود داره و با هم ارتباط خواهند داشت.


مثلا میخواین توزیع شده اجرا کنید، اینجا احتمالات خیلی با هم ارتباط خواهند داشت. ممکنه فکر کنید پیدا کردن اینها و تحلیل الگوریتم براساس این چه جاهایی به کار میاد. این موضوع در رابطه با دیتا سنترهای خیلی بزرگ مثل گوگل و مایکروسافت هست.

یه درصد هم استفاده بهتر از این الگوها و الگوریتم های بهتر ساختن تاثیر داره. و ما  با شرکت های مختلف در ارتباط هستیم که چطوراین هارو بهتر کنیم و به صورت کلی درمورد اینکه چطورارتباط میان سرورها برقرار بشه ؟ 

ما روی داده و پکیج کار می‌کنیم و تو هر کدوم از این اجزا شبکه میریم باید طوری تقسیم بندی کنیم که بتونیم اجرای بهینه تری داشته باشیم چطور بسته هارو دسته بندی کنیم ؟ ما درمورد یه موضوع فقط صحبت می‌کنیم  ولی تمرکز اصلی مون روی متصل کردن این بخش ها هست . 

. ما چرا از این پیشرفت های اخیر در زمینه ml استفاده کنیم؟ من شمارو ارجاع میدم به روش جدیدی که توسعه داده میگه الگوریتم های قدیمی که بودن براساس ماشین لرنینگ خیلی آماری هستن و بعضی جاها اون کارکرد مد نظر ما رو ندارن.

و می‌گیم نود و نه درصد درسته ولی به جاهایی ما نیاز داریم  دقیق دقیق باشه برای همین باید تخریب دقیقی داشته باشیم که در بدترین حالت و در بهترین حالت چطور میتونن باشه و این چیزیه که داره رشد پیدا می‌کنه ما داریم از این توانایی ها استفاده می‌کنیم .

ما میتونیم از توانایی های دو حیطه استفاده کنیم، یه سری الگوریتم هایی هستن که برای توسعه هوش مصنوعی استفاده میشه کدوما هستن ؟
